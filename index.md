---
layout: default
title: 仕様書作成ツール環境構築(Jekyll)
---
# **Chapter 5**

## *Introduction*

　固定された基底関数の線形結合からなるモデルでは「次元の呪い」により大規模な問題には適さない。ゆえに基底関数を奏したものにも使えるようにするために基底関数をデータに適応させる必要がある。  
　この課題に対処するため、SVM(サポートヴェクターマシン)やRVM(リレバンスヴェクターマシン)があり、どちらも固定された基底関数から一部を取り出す手法を採択していて、モデルに含まれる基底関数の数をもとの訓練セットデータの数よりも抑えられる。両者の違いは第7章で詳しく。  

　別のアプローチとしてあらかじめ基底関数の数を固定して、内部の数値のみをいじれるパラメトリックな基底関数をもちいる手法がある。このタイプでパターン認識領域で最も成功しているのがフィードフォワードニューラルネットワーク（多層パーセプトロン）。ただこの「多層パーセプトロン」はやや誤解を招く言い方であり、このモデルは複数のパーセプトロン（0or1の結果を返す）ではなく連続的な非線形性を持つロジスティック回帰モデル（0~1の値をとれる）の重ね合わせ。これにより同党の汎化性能をもつSVMよりはるかに低用量ゆえ処理も高速。ただこの弊害で訓練に使われる尤度関数は凸関数でないため、局所最小が多数存在して最適化は難しくなる（ただしGPUとテクニックでごり押し可能）。  

　ニューラルネットワークは元々生物学的なシステムにおける情報処理を数学的に表現しようとする試みに期限しているがパターン認識の実用的応用の観点から見ると脳の仕組みにどれだけ似ているかを追求することは不要であり、本書では特にもっとも有用性が高いとされる多層パーセプトロンについて議論する。  
この章ではニューラルネットがどのような数式構造をしていて、どのように学習し、どう正則化して、どう応用されるかまで一通りカバーする。

## 5-1 *Feed-Forward Network Functions*  

　固定された非線形基底関数\\(\phi_{j}(x)\\)の線形結合に基づいて、  
\\(y(x, w)=f\\left(\\sum_{j=1}^{M}w_{j}\\phi_{j}(x)\\right)\\)  
このような形式。ここで関数\\(f()\\)は**分類の場合は非線形の活性化関数**、**回帰の場合は恒等関数**となる。われわれはこのモデルを拡張し、基底関数がパラメータ依存にするようにし、訓練中に重み\\(w_{j}\\)とともにこれらのパラメータも調整可能にすること。  
　これによって基本的なニューラルネットワークモデルへと導かれる。このモデルは一連の関数変換で記述できる。  

1.入力変数\\(x_{1}\\)、…\\(x_{D}\\)に対してM個の線形結合を施す  
\\(a_{j}=\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}+w_{j0}^{(1)}\\)  
ここで\\(j=1,…,M\\)、上付きの(1)はそれらのパラメータがネットワークの第1層に属していることを、  
\\(w_{ji}\\)は**重み**を、\\(w_{j0}\\)は**バイアス**と呼ぶ。また\\(a_{j}\\)はactivation。

2.それぞれの\\(a_{j}\\)に対して微分可能な非線形の活性化関数\\(h()\\)を適用することで、次のように変換される。  
\\(z_{j}=h(a_{j})\\)  
これらの\\(z_{j}\\)は隠れユニットである。また非線形関数\\(h()\\)はシグモイド関数が選ばれる。  

3.さらにこれらの値は再び線形結合されて出力ユニットactivationsを与える。  
\\(a_{k}=\sum_{j=1}^{M}w_{kj}^{(2)}z_{j}+w_{k0}^{(2)}\\)  
ここで\\(k=1,…,K\\)であり、出力の総数。この変換はネットワークの第2層に対応し、それぞれ同様に重み、バイアスを含む。  

4.最後に出力ユニットの活性化値\\(a_{k}\\)は適切な活性化関数を用いて変換されてネットワークの出力\\(y_{k}\\)が得られる。  

このようにして、  
**通常の回帰問題**では活性化関数は恒等関数となるから、  
\\(y_{k}=a_{k}\\)  
**複数の二値分類問題**において各出力ユニットのactivationはロジスティックシグモイド関数で変換されて  
\\(y_{k}=\sigma(a_{k})\\)  
ただしシグモイド関数\\(\sigma\\)は  
\\(\sigma(a)=\\frac{1}{1+\\exp(-a)}\\)  
**多クラス分類問題**ではソフトマックス活性化関数が使われる。  
以上をまとめて最終的なネットワーク関数は以下、  
\\(y_{k}(x,w)=\sigma\\left(\\sum_{j=1}^{M}w_{kj}^{(2)}h\\left(\\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}+w_{j0}^{(1)}\\right)+w_{k0}^{(2)}\\right)\\)  
とあらわすことができる。  

　再び言及するが、ニューラルネットワークとMLPの決定的な違いは  
パーセプトロンが**段階的な非線形関数**を使うのに対し、  
ニューラルネットワークでは隠れユニットに**連続的なシグモイド関数の様な非線形関数を用いる**点である。これによってニューラルネットワークの関数はパラメータに対して微分可能になる。そしてこれがネットワークの学習において非常に重要な役割を果たすことになる。  
　仮にネットワークのすべての隠れユニットの活性化関数が線形であると、そのネットワークは常に「隠れユニットなし」で同等のネットワークを構成できることになる（以下で説明）。  
入力：\\(x\\)  
隠れ層：\\(z=w^{(1)}x+b^{(1)}\\)  
出力：\\(y=w^{(2)}z+b^{(2)}\\)  
で活性化関数が線形\\(h(z)=z\\)とすると、  
\\(y=w^{(2)}(w^{(1)}x+b^{(1)})+b^{(2)}\\)  
\\(=(w^{(2)}w^{(1)})x+(w^{(2)}b^{(1)}+b^{(2)})\\)  
となって結局1層の線形変換とバイアスでまとめられる。ゆえに同等のネットワークという表現。  
　このようなネットワークは主成分分析(PCA)（データの次元を削減するための線形手法）として用いられるが、結局それどまりになってしまうためあまり関心がもたれない。  

　ネットワークアーキテクチャの一般化には  
入力→層1→層2→出力  
のような通常のニューラルネットワークフローのほかに、  
入力→→層2→出力（或いは直接出力）  
といったような**スキップ層接続**といった手法もある。理論的にはシグモイド関数を使った隠れユニットを持つネットワークでは入力値がある値に収まっていればスキップ層接続を模倣することが可能ではあるが（シグモイド関数の1に近い部分の入力値の時ってコトかな？）、実際にはスキップ接続を明示的にネットワークに組み込む方が性能や学習効率デメリットがある場合がある。また隠れユニットのすべてが結合されている（全結合(dense)である）という必要もない（畳み込みニューラルネットワークを扱うときにみるっことになる）。  
　今回のフィードフォワードアーキテクチャではネットワーク図とその数学的関数との間に直接的な対応関係があるため教科書に書いてあるような図が書ける。しかしこれはフィードフォワードアーキテクチャが有向サイクルが存在しない一方向に情報が流れるものであるためである。  

　フィードフォワードネットワークは非常に汎用的な性質を持っていることがわかっている。そのためニューラルネットワークは「万能近似器」であるとされている。（類語で万能近似定理：適切な構造とパラメータさえあればニューラルネットワークはどんな連続関数でもほぼ再現できるという定理）  
　例えば線形出力を持つ2層ネットワーク（入力層→隠れ層→出力層）は入力が閉区間にある任意の連続関数を任意の精度で一様近似することが可能。（ただし隠れ層には十分に適切な量の隠れユニットを持っている場合に限る。）  
　そしてこの結果は多くの種類の活性化関数（非線形）で成り立つが、**多項式ではそうはならない**（厳密には多項式を**含む**のはいいけど多項式**だけ**で構成されてはいけない）ことに注意。なぜなら多項式活性化関数のみによって構成される隠れユニットを通過すると出力結果は多項式にしかならないから。詳しくは「多項式全体」は連続関数空間上で稠密（dense）ではないため、どんな連続関数にも無限に近づける保証がないということ。ゆえに隠れ層が多項式関数のみによって構成されていてはいけない。  
　ワイエルシュトラウスの近似定理においては多項式は稠密であるといえるが、万能近似定理で重要なのはどんな入力領域でも任意の関数を近似できる表現力があること、しかし多項式のネットワークで作れる多項式はあくまで「多項式の合成と線形結合」でありこれは結局「多項式」でしかないから周期関数や指数関数などの非多項式関数を完全には表現できない。シグモイド関数のような非多項式関数は多項式では表現できない曲がり方や飽和性を持っている。これがネットワークに真の万能性をもたらす。  
　話を戻すが、万能近似定理の存在は理論的には安心材料になるが実際の問題は与えられた訓練データからどうやって適切なパラメータ（重み）を見つけ出すかという点である。これを見つける手法の例として最尤推定法やベイズ的手法があるが後に解説する。  

### 5-1-1 Weight-space symmetries（重み空間対称性）  

　フィードフォワードネットワークの性質の1つにベイズモデル比較を考えるときに重要になる点がある。それは  
**重みベクトル\\(w\\)の異なる組み合わせが同じ入力→出力の写像を生み出すことがある。**ということ。  
　たとえば  

- 隠れユニットがM個
- 活性化関数はtanh
- 各層のユニットは全結合  

であるときもしある隠れユニットに入ってくるすべての重みとバイアスの符号を反転させるとtanhは機関数であるからこの入力に対するそのユニットの出力の符号も反転する。さらにこのユニットから出てくる重みやバイアスの符号もすべて反転させれば結果的に出力全体は操作前と変化しないことになる。  
　こうした**「符号反転対称性」**は隠れユニットごとに1つずつあり、隠れユニットがM個あるなら\\(2^{M}\\)通りの異なる重みベクトルが同じ関数を示す。  
　また別の対称性として、  
**特定の隠れユニットに入るすべての重みとバイアス、およびそのユニットから出るすべての重みを別の隠れユニットのものと丸ごと入れ替える。**  
これを行ってもネットワークの入力出力マッピングは変化しない。  
　ゆえに隠れユニットがM個ある場合、このような「ユニットに入れ替え」による対称性は\\(M!\\)通りある。つまり隠れユニットを並べ替えるすべての組み合わせに対してそれぞれ異なるけど同じ関数を表す重みベクトルが存在する。  
　以上「符号反転対称性」と「ユニット入れ替え対称性」をまとめると、  

- 全体で\\(2^{M}M!\\)通りの対称性が存在する。  

　3層以上のネットワークでは、それぞれの隠れ層に対して同じような対称性があるため全体の対称性はそれぞれの積の形で表される。これらの対称性は特殊な活性化関数（tanhなど）に限らず多くの種類の活性化関数に共通して起こる性質であり、多くの実用ケースではこの対称性は大きな問題ではないが、この対称性を考慮に入れなければならない具体例が出てくることがある。（のち解説）  

# 5.2 Network Training  

　これまではニューラルネットワークを「入力ベクトルから出力ベクトルまでのパラメトリックな非線形関数の一般的なクラス」としてみてきた。   
ネットワークのパラメータを決定する問題への単純なアプローチは最小二乗法がある。  
\\(E(w)=\frac{1}{2}\sum_{n=1}^{N}||y(x_{n},w)-t_{n}||^{2}\\)  
更にここからニューラルネットワークの出力に確率的な解釈を与えることでより一般的で強力な学習手法を提供することができる。  
　まず回帰問題について考える。ここでは実数値を取る１つの目標変数tを扱い、tが平均\\(y(x,w)\\)（ニューラルネットワークの出力）を持つガウス分布に従うと仮定する。  
　よって、  
\\(p(t|x,w,\beta)=\mathcal{N}(t|y(x,w),\beta^{-1})\\)  
ここで\\(\beta\\)はガウス雑音の制度（分散の逆数）である。  
\\(p(t|x,w)\\)は条件付き確率密度関数を表しており、入力ベクトルxとモデルパラメータwが与えられたときの出力（ターゲット）tの確率を意味する。  
また、\\(\mathcal{N}(t|y(x,w),\beta^{-1})\\)は正規分布を表していて、平均\\(\mu=y(x,w)\\)はニューラルネットワークの出力、分散\\(\sigma^{2}=\beta^{-1}\\)で、つまりニューラルネットワークの出力を平均として、分散\\(\beta^{-1}\\)の正規分布からターゲットtがサンプルされるという過程に基づいている。  
　この式を用いてニューラルネットワークの出力が活性化関数\\(y(x,w)\\)の形で与えられた任意の関数である限り、回帰問題を取り扱うことができる。  
　独立に同一分布したデータセット\\(D=({x_{n},t_{n}})\\)を与えられた時、対応する尤度関数は、  
\\(p(t|X,w,\beta)=\prod_{n=1}^{N}p(t_{n}|x_{n},w,\beta)\\)  
と書くことができて、（尤度関数とは「あるパラメータw,\\(\beta\\)のもとで、観測されたデータ\\(\mathcal{D}\\)が得られる確からしさ」を表す関数であり、今回データが独立に得られているから「全体の尤度＝各データ点の確率の積で表される。）  
これの対数を取って符号を反転させて得られる負の対数尤度が以下のエラー関数となる。  
\\(\frac{\beta}{2}\sum_{n=1}^{N}(y(x_{n},w)-t_{n})^{2}-\frac{N}{2}\ln\beta+\frac{N}{2}\ln(2\pi)\\)  
この関数はパラメータw及び\\(\beta\\)を学習する際に用いられる。  
ニューラルネットワークの文献では尤度関数の最大化ではなく、その対数の負をエラー関数として最小化する慣習があるため、ここでもそれに従う。（積の形で扱うより和の形で扱うほうがデータを取りやすく、勾配降下法などのアルゴリズムと親和性が高いから。）  
まずwについての決定論的な推定を考えると尤度の最大化は以下の２乗誤差関数を最小化することと同値である。（なおここでは定数項やスケーリング関数を無視している。）  
\\(E(w)=\frac{1}{2}\sum_{n=1}^{N}(y(x_{n},w)-t_{n})^{2}\\)  
E(w)を最小化することによって得られるwの値は最大化される尤度に対応するため、これを\\(w_{ML}\\)と表記する。  
\\(w_{ML}\\)を求めたら、次にノイズの精度\\(\beta\\)の値を負の対数尤度を最小化することで求める。以下がその式。  
\\(\frac{1}{\beta_{ML}}=\frac{1}{N}\sum_{n=1}^{N}{y(x_{n},w_{ML})-t_{n}}^{2}\\)  
もし目標変数が複数ある場合にはそれらがxとwに条件付けられて互いに独立であり、かつノイズの精度\\(\beta\\)を共有していると仮定する。この時ターゲット値の条件付き分布は  
\\(p(t|x,w)=N(t|y(x,w),\beta^{-1}I)\\)  
となる。そうして単一のターゲット変数に対して行ったのと同じ議論に従うと、最尤推定による重みは二乗誤差関数の最小化によって求められる。するとノイズの精度は以下。  
\\(\frac{1}{\beta_{ML}}=\frac{1}{NK}\sum_{n=1}^{N}||y(x_{n},w_{ML})-t_{n}||^{2}\\)  
ここでKはターゲット変数の個数である。  

さて、今度は２クラス分類（binary classification）について考える。このときターゲット変数tはt=1ならクラス\\(C_{1}\\)、t=0なら\\(C_{2}\\)を表すとする。  
出力が一つでその活性化関数がロジスティックシグモイド関数であるようなネットワークを考える：  
\\(y=\sigma(a)=\frac{1}{1+\exp(-a)}\\)  
このとき、\\(0<y(x,w)\leq1\\)が成り立ち、この式から\\(y(x,w)\\)は条件付き確率\\(p(C_{1}|x)\\)として解釈でき（入力xがあたえられたときにクラス\\(C_{1}\\)に属する確率という解釈をするから「条件付き」確率という考え方）、\\(p(C_{2}|x)\\)は\\(1-y(x,w)\\)で与えられる。  
与えられた入力に対するターゲットtの条件付き確率は以下のベルヌーイ分布で与えられる。  
\\(p(t|x,w)=y(x,w)^{t}(1-y(x,w))^{1-t}\\)  
もし観測値が互いに独立であるトレーニングセットを考えるならば、誤差関数（負の対数尤度で与えられる）はクロスエントロピー誤差関数となる。その形は以下に示す。  
\\(E(w)=-\sum_{n=1}^{N}(t_{n}\ln(y_{n})+(1-t_{n})\ln(1-y_{n}))\\)  
ここで\\(y_{n}\\)は\\(y(x_{n},w)\\)を表す。  
この時注意すべきは回帰のときにあった「ノイズの精度\\(\beta\\)」に対応するものは分類には存在しない。これはターゲット値\\(t_{n}\\)方だし比べる付けされていると仮定されているため。（ただしこのモデルはラベル付エラー（つまり誤差付き）にも簡単に拡張できる）  
分類問題において、二乗誤差ではなくクロスエントロピー誤差を使うことで学習がより早く進むのみならず、汎化性能も向上するらしい。  
（そもそもクロスエントロピー誤差を用いる手法とは、正しいクラスに対して高い確率を出すように学習が進むように設計された手法。一方最小二乗法は値としてどれだけズレているかを見る手法。前者は分類向きで後者は回帰向きである。）  

もしK個の別々の２クラス分類を行う場合、それぞれがロジスティックシグモイド活性化関数を持つK個の出力ユニットを持つニューラルネットワークを使える。それぞれの出力には２クラスラベル\\(t_{k}\in(0,1)(k=1,...,K)\\)が対応する。  
入力ラベルが与えられた時、各クラスラベルが互いに独立であると仮定するとターゲットの条件付き確率は  
\\(p(t|x,w)=\prod_{k=1}^{K}y_{k}(x,w)^{t_{k}}(1-y_{k}(x,w))^{1-t_{k}}\\)  
と表される。  
先程と同様の手順でこの尤度関数の負の対数を取ることで以下のような誤差関数が得られる。  
\\(E(w)=-\sum_{n-1}^{N}\sum_{k=1}^{K}(t_{nk}\ln(y_{nk})+(1-t_{nk})\ln(1-y_{nk}))\\)  
ここで\\(y_{nk}\\)は\\(y(x_{n},w)\\)を表している。  
このニューラルネットワークによる解法と線形分類モデルに基づく解法を比較する。例えば図5-1に示された２層ニューラルネットワークで考えるとき、第１層の重みパラメータはすべての出力ユニット間で共有されている。一方で線形モデルでは各分類問題は独立に解かれる。  
どういうことかというと、簡潔に言うと  
NNは「特徴を共通で学ぶ」  
けれど、線形モデルは「各クラスごとに別々に学ぶ」  
より詳しくいうと、顔認識を例に取る。  
- ニューラルネットワーク  
入力：顔画像  
第１層：目、鼻、口みたいな「抽出特徴」を学ぶ  
出力層：その特徴を使って「AさんorBさんorCさん...」を決定する  
→すべての出力クラス（A,B,C,...）が同じ「目、鼻、口」の特徴を見ている
→つまり第１層の重みは「全ての出力ユニット間で共有されている」  
- 線形モデル  
クラスごと（Aさん、Bさん、Cさん...）で独立のモデルを持つ  
→「Aさんの判定用の重み」「Bさんの～」「Cさんの～」を別々に学ぶ  
→つまり各出力クラスがそれぞれ独立した評価基準（入力からの重みベクトル）をもっている  
→特徴を共有していない  

以上からNNは特徴量を共有できるため学習効率が良く、学習データが少なくても汎化しやすく、抽象的な中間特徴を自動で学習できるというメリットが有る。（ここが厄介でNNは人間が説明することが難しい特徴量を自動で感じ取ってしまうからブラックボックス問題を生む。）  

