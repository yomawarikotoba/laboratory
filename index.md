---
layout: default
title: 仕様書作成ツール環境構築(Jekyll)
---
# **Chapter 5**

## *Introduction*

　固定された基底関数の線形結合からなるモデルでは「次元の呪い」により大規模な問題には適さない。ゆえに基底関数を奏したものにも使えるようにするために基底関数をデータに適応させる必要がある。  
　この課題に対処するため、SVM(サポートヴェクターマシン)やRVM(リレバンスヴェクターマシン)があり、どちらも固定された基底関数から一部を取り出す手法を採択していて、モデルに含まれる基底関数の数をもとの訓練セットデータの数よりも抑えられる。両者の違いは第7章で詳しく。  

　別のアプローチとしてあらかじめ基底関数の数を固定して、内部の数値のみをいじれるパラメトリックな基底関数をもちいる手法がある。このタイプでパターン認識領域で最も成功しているのがフィードフォワードニューラルネットワーク（多層パーセプトロン）。ただこの「多層パーセプトロン」はやや誤解を招く言い方であり、このモデルは複数のパーセプトロン（0or1の結果を返す）ではなく連続的な非線形性を持つロジスティック回帰モデル（0~1の値をとれる）の重ね合わせ。これにより同党の汎化性能をもつSVMよりはるかに低用量ゆえ処理も高速。ただこの弊害で訓練に使われる尤度関数は凸関数でないため、局所最小が多数存在して最適化は難しくなる（ただしGPUとテクニックでごり押し可能）。  

　ニューラルネットワークは元々生物学的なシステムにおける情報処理を数学的に表現しようとする試みに期限しているがパターン認識の実用的応用の観点から見ると脳の仕組みにどれだけ似ているかを追求することは不要であり、本書では特にもっとも有用性が高いとされる多層パーセプトロンについて議論する。  
この章ではニューラルネットがどのような数式構造をしていて、どのように学習し、どう正則化して、どう応用されるかまで一通りカバーする。

## 5-1 *Feed-Forward Network Functions*  

　固定された非線形基底関数\\(\phi_{j}(x)\\)の線形結合に基づいて、  
\\(y(x, w)=f\\left(\\sum_{j=1}^{M}w_{j}\\phi_{j}(x)\\right)\\)  
このような形式。ここで関数\\(f()\\)は**分類の場合は非線形の活性化関数**、**回帰の場合は恒等関数**となる。われわれはこのモデルを拡張し、基底関数がパラメータ依存にするようにし、訓練中に重み\\(w_{j}\\)とともにこれらのパラメータも調整可能にすること。  
　これによって基本的なニューラルネットワークモデルへと導かれる。このモデルは一連の関数変換で記述できる。  

1.入力変数\\(x_{1}\\)、…\\(x_{D}\\)に対してM個の線形結合を施す  
\\(a_{j}=\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}+w_{j0}^{(1)}\\)  
ここで\\(j=1,…,M\\)、上付きの(1)はそれらのパラメータがネットワークの第1層に属していることを、  
\\(w_{ji}\\)は**重み**を、\\(w_{j0}\\)は**バイアス**と呼ぶ。また\\(a_{j}\\)はactivation。

2.それぞれの\\(a_{j}\\)に対して微分可能な非線形の活性化関数\\(h()\\)を適用することで、次のように変換される。  
\\(z_{j}=h(a_{j})\\)  
これらの\\(z_{j}\\)は隠れユニットである。また非線形関数\\(h()\\)はシグモイド関数が選ばれる。  

3.さらにこれらの値は再び線形結合されて出力ユニットactivationsを与える。  
\\(a_{k}=\sum_{j=1}^{M}w_{kj}^{(2)}z_{j}+w_{k0}^{(2)}\\)  
ここで\\(k=1,…,K\\)であり、出力の総数。この変換はネットワークの第2層に対応し、それぞれ同様に重み、バイアスを含む。  

4.最後に出力ユニットの活性化値\\(a_{k}\\)は適切な活性化関数を用いて変換されてネットワークの出力\\(y_{k}\\)が得られる。  

このようにして、  
**通常の回帰問題**では活性化関数は恒等関数となるから、  
\\(y_{k}=a_{k}\\)  
**複数の二値分類問題**において各出力ユニットのactivationはロジスティックシグモイド関数で変換されて  
\\(y_{k}=\sigma(a_{k})\\)  
ただしシグモイド関数\\(\sigma\\)は  
\\(\sigma(a)=\\frac{1}{1+\\exp(-a)}\\)  
**多クラス分類問題**ではソフトマックス活性化関数が使われる。  
以上をまとめて最終的なネットワーク関数は以下、  
\\(y_{k}(x,w)=\sigma\\left(\\sum_{j=1}^{M}w_{kj}^{(2)}h\\left(\\sum_{i=1}^{D}w_{ji}^{(1)}x_{i}+w_{j0}^{(1)}\\right)+w_{k0}^{(2)}\\right)\\)  
とあらわすことができる。  

　再び言及するが、ニューラルネットワークとMLPの決定的な違いは  
パーセプトロンが**段階的な非線形関数**を使うのに対し、  
ニューラルネットワークでは隠れユニットに**連続的なシグモイド関数の様な非線形関数を用いる**点である。これによってニューラルネットワークの関数はパラメータに対して微分可能になる。そしてこれがネットワークの学習において非常に重要な役割を果たすことになる。  
　仮にネットワークのすべての隠れユニットの活性化関数が線形であると、そのネットワークは常に「隠れユニットなし」で同等のネットワークを構成できることになる（以下で説明）。  
入力：\\(x\\)  
隠れ層：\\(z=w^{(1)}x+b^{(1)}\\)  
出力：\\(y=w^{(2)}z+b^{(2)}\\)  
で活性化関数が線形\\(h(z)=z\\)とすると、  
\\(y=w^{(2)}(w^{(1)}x+b^{(1)})+b^{(2)}\\)  
\\(=(w^{(2)}w^{(1)})x+(w^{(2)}b^{(1)}+b^{(2)})\\)  
となって結局1層の線形変換とバイアスでまとめられる。ゆえに同等のネットワークという表現。  
　このようなネットワークは主成分分析(PCA)（データの次元を削減するための線形手法）として用いられるが、結局それどまりになってしまうためあまり関心がもたれない。  

　ネットワークアーキテクチャの一般化には  
入力→層1→層2→出力  
のような通常のニューラルネットワークフローのほかに、  
入力→→層2→出力（或いは直接出力）  
といったような**スキップ層接続**といった手法もある。理論的にはシグモイド関数を使った隠れユニットを持つネットワークでは入力値がある値に収まっていればスキップ層接続を模倣することが可能ではあるが（シグモイド関数の1に近い部分の入力値の時ってコトかな？）、実際にはスキップ接続を明示的にネットワークに組み込む方が性能や学習効率デメリットがある場合がある。また隠れユニットのすべてが結合されている（全結合(dense)である）という必要もない（畳み込みニューラルネットワークを扱うときにみるっことになる）。  
　今回のフィードフォワードアーキテクチャではネットワーク図とその数学的関数との間に直接的な対応関係があるため教科書に書いてあるような図が書ける。しかしこれはフィードフォワードアーキテクチャが有向サイクルが存在しない一方向に情報が流れるものであるためである。  

　フィードフォワードネットワークは非常に汎用的な性質を持っていることがわかっている。そのためニューラルネットワークは「万能近似器」であるとされている。（類語で万能近似定理：適切な構造とパラメータさえあればニューラルネットワークはどんな連続関数でもほぼ再現できるという定理）  
　例えば線形出力を持つ2層ネットワーク（入力層→隠れ層→出力層）は入力が閉区間にある任意の連続関数を任意の精度で一様近似することが可能。（ただし隠れ層には十分に適切な量の隠れユニットを持っている場合に限る。）  
　そしてこの結果は多くの種類の活性化関数（非線形）で成り立つが、**多項式ではそうはならない**（厳密には多項式を**含む**のはいいけど多項式**だけ**で構成されてはいけない）ことに注意。なぜなら多項式活性化関数のみによって構成される隠れユニットを通過すると出力結果は多項式にしかならないから。詳しくは「多項式全体」は連続関数空間上で稠密（dense）ではないため、どんな連続関数にも無限に近づける保証がないということ。ゆえに隠れ層が多項式関数のみによって構成されていてはいけない。  
　ワイエルシュトラウスの近似定理においては多項式は稠密であるといえるが、万能近似定理で重要なのはどんな入力領域でも任意の関数を近似できる表現力があること、しかし多項式のネットワークで作れる多項式はあくまで「多項式の合成と線形結合」でありこれは結局「多項式」でしかないから周期関数や指数関数などの非多項式関数を完全には表現できない。シグモイド関数のような非多項式関数は多項式では表現できない曲がり方や飽和性を持っている。これがネットワークに真の万能性をもたらす。  
　話を戻すが、万能近似定理の存在は理論的には安心材料になるが実際の問題は与えられた訓練データからどうやって適切なパラメータ（重み）を見つけ出すかという点である。これを見つける手法の例として最尤推定法やベイズ的手法があるが後に解説する。  

### 5-1-1 Weight-space symmetries（重み空間対称性）  

　フィードフォワードネットワークの性質の1つにベイズモデル比較を考えるときに重要になる点がある。それは  
**重みベクトル\\(w\\)の異なる組み合わせが同じ入力→出力の写像を生み出すことがある。**ということ。  
　たとえば  

- 隠れユニットがM個
- 活性化関数はtanh
- 各層のユニットは全結合  

であるときもしある隠れユニットに入ってくるすべての重みとバイアスの符号を反転させるとtanhは機関数であるからこの入力に対するそのユニットの出力の符号も反転する。さらにこのユニットから出てくる重みやバイアスの符号もすべて反転させれば結果的に出力全体は操作前と変化しないことになる。  
　こうした**「符号反転対称性」**は隠れユニットごとに1つずつあり、隠れユニットがM個あるなら\\(2^{M}\\)通りの異なる重みベクトルが同じ関数を示す。  
　また別の対称性として、  
**特定の隠れユニットに入るすべての重みとバイアス、およびそのユニットから出るすべての重みを別の隠れユニットのものと丸ごと入れ替える。**  
これを行ってもネットワークの入力出力マッピングは変化しない。  
　ゆえに隠れユニットがM個ある場合、このような「ユニットに入れ替え」による対称性は\\(M!\\)通りある。つまり隠れユニットを並べ替えるすべての組み合わせに対してそれぞれ異なるけど同じ関数を表す重みベクトルが存在する。  
　以上「符号反転対称性」と「ユニット入れ替え対称性」をまとめると、  

- 全体で\\(2^{M}M!\\)通りの対称性が存在する。  

　3層以上のネットワークでは、それぞれの隠れ層に対して同じような対称性があるため全体の対称性はそれぞれの積の形で表される。これらの対称性は特殊な活性化関数（tanhなど）に限らず多くの種類の活性化関数に共通して起こる性質であり、多くの実用ケースではこの対称性は大きな問題ではないが、この対称性を考慮に入れなければならない具体例が出てくることがある。（のち解説）  

# 5.2 Network Training  

　これまではニューラルネットワークを「入力ベクトルから出力ベクトルまでのパラメトリックな非線形関数の一般的なクラス」としてみてきた。   
ネットワークのパラメータを決定する問題への単純なアプローチは最小二乗法がある。  
\\(E(w)=\frac{1}{2}\sum_{n=1}^{N}||y(x_{n},w)-t_{n}||^{2}\\)  
更にここからニューラルネットワークの出力に確率的な解釈を与えることでより一般的で強力な学習手法を提供することができる。  
　まず回帰問題について考える。ここでは実数値を取る１つの目標変数tを扱い、tが平均\\(y(x,w)\\)（ニューラルネットワークの出力）を持つガウス分布に従うと仮定する。  
　よって、  
\\(p(t|x,w,\beta)=\mathcal{N}(t|y(x,w),\beta^{-1})\\)  
ここで\\(\beta\\)はガウス雑音の制度（分散の逆数）である。  
\\(p(t|x,w)\\)は条件付き確率密度関数を表しており、入力ベクトルxとモデルパラメータwが与えられたときの出力（ターゲット）tの確率を意味する。  
また、\\(\mathcal{N}(t|y(x,w),\beta^{-1})\\)は正規分布を表していて、平均\\(\mu=y(x,w)\\)はニューラルネットワークの出力、分散\\(\sigma^{2}=\beta^{-1}\\)で、つまりニューラルネットワークの出力を平均として、分散\\(\beta^{-1}\\)の正規分布からターゲットtがサンプルされるという過程に基づいている。  
　この式を用いてニューラルネットワークの出力が活性化関数\\(y(x,w)\\)の形で与えられた任意の関数である限り、回帰問題を取り扱うことができる。  
　独立に同一分布したデータセット\\(D=({x_{n},t_{n}})\\)を与えられた時、対応する尤度関数は、  
\\(p(t|X,w,\beta)=\prod_{n=1}^{N}p(t_{n}|x_{n},w,\beta)\\)  
と書くことができて、（尤度関数とは「あるパラメータw,\\(\beta\\)のもとで、観測されたデータ\\(\mathcal{D}\\)が得られる確からしさ」を表す関数であり、今回データが独立に得られているから「全体の尤度＝各データ点の確率の積で表される。）  
これの対数を取って符号を反転させて得られる負の対数尤度が以下のエラー関数となる。  
\\(\frac{\beta}{2}\sum_{n=1}^{N}(y(x_{n},w)-t_{n})^{2}-\frac{N}{2}\ln\beta+\frac{N}{2}\ln(2\pi)\\)  
この関数はパラメータw及び\\(\beta\\)を学習する際に用いられる。  
ニューラルネットワークの文献では尤度関数の最大化ではなく、その対数の負をエラー関数として最小化する慣習があるため、ここでもそれに従う。（積の形で扱うより和の形で扱うほうがデータを取りやすく、勾配降下法などのアルゴリズムと親和性が高いから。）  
まずwについての決定論的な推定を考えると尤度の最大化は以下の２乗誤差関数を最小化することと同値である。（なおここでは定数項やスケーリング関数を無視している。）  
\\(E(w)=\frac{1}{2}\sum_{n=1}^{N}(y(x_{n},w)-t_{n})^{2}\\)  
E(w)を最小化することによって得られるwの値は最大化される尤度に対応するため、これを\\(w_{ML}\\)と表記する。  
\\(w_{ML}\\)を求めたら、次にノイズの精度\\(\beta\\)の値を負の対数尤度を最小化することで求める。以下がその式。  
\\(\frac{1}{\beta_{ML}}=\frac{1}{N}\sum_{n=1}^{N}(y(x_{n},w_{ML})-t_{n})^{2}\\)  
もし目標変数が複数ある場合にはそれらがxとwに条件付けられて互いに独立であり、かつノイズの精度\\(\beta\\)を共有していると仮定する。この時ターゲット値の条件付き分布は  
\\(p(t|x,w)=N(t|y(x,w),\beta^{-1}I)\\)  
となる。そうして単一のターゲット変数に対して行ったのと同じ議論に従うと、最尤推定による重みは二乗誤差関数の最小化によって求められる。するとノイズの精度は以下。  
\\(\frac{1}{\beta_{ML}}=\frac{1}{NK}\sum_{n=1}^{N}||y(x_{n},w_{ML})-t_{n}||^{2}\\)  
ここでKはターゲット変数の個数である。  
負の対数尤度で与えられる誤差関数と出力ユニットの活性化関数には自然な対応関係があり（先程の回帰の部分で尤度関数を負の対数尤度に変化させた時最小二乗法の形が出てきたように出力（回帰、２クラス分類、多クラス分類）の確率分布モデルが決まるとある程度一意に活性化関数が決まり、一位に誤差関数も決まるからこう言える。）、  
回帰の場合、ネットワークの出力活性化関数を恒等関数とみなすことができ、  
\\(y_{k}=a_{k}\\)となる。  
そして対応する二乗誤差関数は次の性質を持つ。  
\\(\frac{\partial E}{\partial a_{k}}=y_{k}-t_{k}\\)  
この性質は誤差の逆伝播（back propagation）について議論するSection5-3にて利用する。

さて、今度は２クラス分類（binary classification）について考える。このときターゲット変数tはt=1ならクラス\\(C_{1}\\)、t=0なら\\(C_{2}\\)を表すとする。  
出力が一つでその活性化関数がロジスティックシグモイド関数であるようなネットワークを考える：  
\\(y=\sigma(a)=\frac{1}{1+\exp(-a)}\\)  
このとき、\\(0<y(x,w)\leq1\\)が成り立ち、この式から\\(y(x,w)\\)は条件付き確率\\(p(C_{1}|x)\\)として解釈でき（入力xがあたえられたときにクラス\\(C_{1}\\)に属する確率という解釈をするから「条件付き」確率という考え方）、\\(p(C_{2}|x)\\)は\\(1-y(x,w)\\)で与えられる。  
与えられた入力に対するターゲットtの条件付き確率は以下のベルヌーイ分布で与えられる。  
\\(p(t|x,w)=y(x,w)^{t}(1-y(x,w))^{1-t}\\)  
もし観測値が互いに独立であるトレーニングセットを考えるならば、誤差関数（負の対数尤度で与えられる）はクロスエントロピー誤差関数となる。その形は以下に示す。  
\\(E(w)=-\sum_{n=1}^{N}(t_{n}\ln(y_{n})+(1-t_{n})\ln(1-y_{n}))\\)  
ここで\\(y_{n}\\)は\\(y(x_{n},w)\\)を表す。  
この時注意すべきは回帰のときにあった「ノイズの精度\\(\beta\\)」に対応するものは分類には存在しない。これはターゲット値\\(t_{n}\\)方だし比べる付けされていると仮定されているため。（ただしこのモデルはラベル付エラー（つまり誤差付き）にも簡単に拡張できる）  
分類問題において、二乗誤差ではなくクロスエントロピー誤差を使うことで学習がより早く進むのみならず、汎化性能も向上するらしい。  
（そもそもクロスエントロピー誤差を用いる手法とは、正しいクラスに対して高い確率を出すように学習が進むように設計された手法。一方最小二乗法は値としてどれだけズレているかを見る手法。前者は分類向きで後者は回帰向きである。）  

もしK個の別々の２クラス分類を行う場合、それぞれがロジスティックシグモイド活性化関数を持つK個の出力ユニットを持つニューラルネットワークを使える。それぞれの出力には２クラスラベル\\(t_{k}\in(0,1)(k=1,...,K)\\)が対応する。  
入力ラベルが与えられた時、各クラスラベルが互いに独立であると仮定するとターゲットの条件付き確率は  
\\(p(t|x,w)=\prod_{k=1}^{K}y_{k}(x,w)^{t_{k}}(1-y_{k}(x,w))^{1-t_{k}}\\)  
と表される。  
先程と同様の手順でこの尤度関数の負の対数を取ることで以下のような誤差関数が得られる。  
\\(E(w)=-\sum_{n-1}^{N}\sum_{k=1}^{K}(t_{nk}\ln(y_{nk})+(1-t_{nk})\ln(1-y_{nk}))\\)  
ここで\\(y_{nk}\\)は\\(y(x_{n},w)\\)を表している。  
このニューラルネットワークによる解法と線形分類モデルに基づく解法を比較する。例えば図5-1に示された２層ニューラルネットワークで考えるとき、第１層の重みパラメータはすべての出力ユニット間で共有されている。一方で線形モデルでは各分類問題は独立に解かれる。  
どういうことかというと、簡潔に言うと  
NNは「特徴を共通で学ぶ」  
けれど、線形モデルは「各クラスごとに別々に学ぶ」  
より詳しくいうと、顔認識を例に取る。  
- ニューラルネットワーク  
入力：顔画像  
第１層：目、鼻、口みたいな「抽出特徴」を学ぶ  
出力層：その特徴を使って「AさんorBさんorCさん...」を決定する  
→すべての出力クラス（A,B,C,...）が同じ「目、鼻、口」の特徴を見ている
→つまり第１層の重みは「全ての出力ユニット間で共有されている」  
- 線形モデル  
クラスごと（Aさん、Bさん、Cさん...）で独立のモデルを持つ  
→「Aさんの判定用の重み」「Bさんの～」「Cさんの～」を別々に学ぶ  
→つまり各出力クラスがそれぞれ独立した評価基準（入力からの重みベクトル）をもっている  
→特徴を共有していない  

以上からNNは特徴量を共有できるため学習効率が良く、学習データが少なくても汎化しやすく、抽象的な中間特徴を自動で学習できるというメリットが有る。（ここが厄介でNNは人間が説明することが難しい特徴量を自動で感じ取ってしまうからブラックボックス問題を生む。）  

最後に、各入力がK個の互いに排他的なクラスのうち１つに割り当てられる典型的な多クラス分類問題を考える。このときターゲット変数\\(t_{k}\in(0,1)\\)はどのクラスに属するかを示す1-of-K表現（１つだけが１で他は０）で与えられ、ネットワークの出力は\\(y_{k}(x,w)=p(t_{k}=1|x)\\)と解釈される。  
このとき誤差関数は以下のようになる。  
\\(E(w)=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\ln(y_{k}(x,w))\\)  
この出力ユニットの活性化関数はsoftmax関数が妥当であり、正準リンク関数に対応している。  
(まず正準リンク関数とは統計モデリング（特に一般化線形モデル）において「確率分布と活性化関数をつなぐ橋渡し的なもの」であり、ある出力変数の確率分布が指数型分布に属している時、対数尤度が特に簡単になる関数のこと。  
またsoftmax関数とは多クラス分類において出力が確率になるように整える関数。)  
softmax関数は次のように表される。  
\\(y_{k}(x,w)=\frac{\exp(a_{k}(x,w))}{\sum_{j}\exp(a_{j}(x,w))}\\)  
そしてこの関数は以下の性質を満たす。  
- \\(0\leq{y_{k}}\leq{1}（すべての出力が確率）\\)
- \\(\sum_{k}y_{k}=1（全てのクラスの出力の和が1）\\)  
さてここで見覚えのある関数の型地が出てきたことにお気づきだろうか？  
そう統計力学におけるボルツマン分布である。  
ボルツマン分布はエネルギー状態\\(E_{i}\\)をもつ各状態iに対してその状態が選ばれる確率を  
\\(p_{i}=\frac{\exp(-\betaE_{i})}{Z}　ただしZ=\sum_{j}\exp(-\betaE_{j})\\)  
- \\(\beta=\frac{1}{k_{B}T}\\)：温度と関係する定数（逆温度）  
- Z：分配関数（確率を正規化する係数）  
であった。  
### 比較図  
|ボルツマン分布|softmax関数|  
|:---:|:---:|  
|\\(\exp(-\betaE_{i})\\)|\\(\exp(a_{k})\\)|  
|状態iのエネルギー|出力ユニットkのスコア|  
|確率\\(p_{i}\\)|確率\\(y_{k}\\)|  
|\\(Z=\sum_{j}\exp(-{\beta}E_{j})\\)|\\(\sum_{j}\exp(a_{j})\\)|  
という対応関係になり、つまりsoftmax関数は「スコア\\(a_{k}\\)を**エネルギー**とみなしたときのボルツマン分布という解釈ができる。ただし、統計力学では低エネルギーが高確率だから\\(-\betaE_{i}\\)、softmax関数ではスコアが高いほど高確率だから\\(+a_{k}\\)というふうに符号が反転しているだけ。  


注意としてすべての\\(a_{k}\\)に定数を加えても\\(y_{k}(x,w)\\)の値は変わらないため、重み空間内でのエラー関数が変化しないという問題が起こりえる（これを縮退と呼ぶ）。もう少し噛み砕くと「activationsを変化させる」＝「重み空間上でエラー関数をある方向に動かす」ことをしてもエラーが減らないような方向（＝無駄な自由度）があるという現象が縮退。  
これを回避するために正則化項をエラー関数に加える必要があるらしいが詳しくはSection5-5にて。  
ここで再び出力ユニットの活性化に関する誤差関数の導関数は回忌の場合と同じように  
\\(\frac{\partial{E}}{\partial{a_{k}}}=y_{k}-t_{k}\\)となる。  

まとめると問題のタイプに応じて出力ユニットの活性化関数と誤差関数を自然に対応させる選択が存在する。  
- 回帰：
線形関数　＋　二乗誤差関数  
- ２クラス分類（複数の独立した分類）：  
ロジスティックシグモイド関数　＋　クロスエントロピー誤差関数  
- 多クラス分類（それぞれ排他的なクラス）：  
softmax関数　＋　多クラス用クロスエントロピー誤差関数  

また、複数クラスを含む分類問題では：
- 単一のシグモイド出力を使って１クラスずつ学習してもよいし  
- softmax関数で一括して分類しても良い  

ちなみにこれは多ラベル分類という手法で例えば、「人」「山」「犬」が同時に写っている画像を入力したとして、その３つ以外にも様々な種類のクラスを定義しておくことで出力時にそれら複数のラベルを同時に予測してくれるようなモデルのこと。  
基本的なやり方は前者の\\(sigmoid×クラス数\\)が定石。  
## 5.2.1 Prameter optimization  
ここでは、選ばれた誤差関数\\(E(w)\\)を最小にする重みベクトルwを見つける課題に取り組む。誤差関数は重み空間に乗った地形のようなものであると考えることができ、図5-5に示されている。  
まず重み空間において\\(w→w+\delta{w}\\)へと微小量動かした時、誤差関数の変化は  
\\(\delta{E}\simeq\delta{w^{T}}\nabla{E(w)}\\)  
と表される。ここでベクトル\\(\nabla{E(w)}\\)は誤差関数が最も急速に増加する方向を示す。そして誤差関数E(w)はwに関する滑らかで連続的な関数であるため、その最小値は勾配が⁰になる重み空間上の点で表すことができる。  
したがって最小値は、  
\\(\nabla{E(w)}=0\\)  
となる点で現れる。勾配がゼロになる点は停留点と呼び、極小点、極大点、鞍点のいずれかに分類される。  
我々の目標はエラー関数E(w)が最小となるようなwを探すことである。しかし誤差関数は一般に重みやバイアスの値に対して非常に非線形な依存性を持つから、勾配がゼロになる点が重み空間上に多数存在することになる。  
実際にセクション5-1-1より、任意の局所的最小値にあるwに対して、その周囲には同じマッピング関数を持つ別の極小点が多数存在する。例えば図5-1のような２層ニューラルネットワークで隠れユニットがM個ある場合、それぞれの最小点は\\(M!2^{M}\\)個の同等な点の集合に属している。  
（これどういうことかというと、結論としては等価な局所最小点（値もマッピングも正しい）が複数箇所にあるということ。つまり符号反転対称性やユニット入れ替え対称性によって勾配ベクトルの方向や曲率といったグラフの概形や最小値の座標は相異なる（より詳しく言うと鏡像的に異なる）が深さの程度（つまり最小値）や意味上の概形は同じ谷（エラー関数）がいくつも存在している状況ということ）  
さらに、実際には互いに異なる多数の曲商店が存在するのが一般的。誤差関数の最小値を与える最小点が**グローバルミニマム**とよばれ、それ以外の極小点は**ローカルミニマム**と呼ばれる。ニューラルネットワークを機能させるには必ずしもグローバルミニマムを見つける必要はなく（通常それがグローバルミニマムかどうか知る方法もないため）、複数のローカルミニマムを比較して、十分に良い貝を見つける必要がある。  
このように解析的に勾配がゼロとなる解を見つけることは解析的にはほとんど不可能であるため、我々は数値的な反復手法を用いることになる。連続的な非線形関数の最適化は広く研究されておりそれを効率的に解くための豊富な文献も存在する。  
（ちなみにエラー関数は基本数百から数千もの多次元変数関数になることが多く、高次元空間を人間が見て最小と判断するのは不可能であるため「解析的にはほとんど不可能という言葉を使った」）  
一般的な方法ではまず初期値\\(w^{(0)}\\)を設定し、その後以下の形式で重み空間内を移動する。  
\\(w^{(\tau+1)}=w^{(\tau)}+\delta{w}^{(\tau)}\\)  
ここで\\(\tau\\)は反復ステップを表している、アルゴリズムによって更新ステップ\\(\delta{w}^{(\tau)}\\)の選び方は異なる。多くのアルゴリズムでは勾配情報を利用し、各更新後には\\(\delta{E(w)}\\)の値を新たな重みベクトル\\(w^{(\tau+1)}\\)に対して再評価する必要がある。  
## 5.2.2 Local quadratic approximation  
誤差関数に対して局所的な二次近似を考えると、最適化問題に対する洞察や、問題を解くための様々な手法に関する理解を深められる。  
誤差関数E(w)を重み空間上のある点\\(\hat{w}\\)の周りでテイラー展開すると  
\\(E(w) \simeq {E(\hat{w})+(w-\hat{w})^{T}b+\frac{1}{2}(w-\hat{w})^{T}H(w-\hat{w})}\\)  
この様になる。（ただし、三次以降の項は省略されている。）  
またベクトルbは点\\(\hat{w}\\)における誤差関数の勾配で表される。  
\\(b={\nabla{E}}\\)（ただし\\(w=\hat{w}\\)）  
そしてヘッセ行列Hは勾配のヤコビ行列（＝二階微分）であり、次のような要素を持つ。  
\\(H_{ij} \equiv {\frac{\partial^{2} E}{\partial{w_{i}}\partial{w_{j}}}|_{w=\hat{w}}}\\)  
テイラー展開の式よりで両辺wに関する偏微分を行うことによって、勾配の局所近似は次のように考えられる。  
\\(\nabla E\simeq{b+H(w-\hat{w})}\\)  
点wが点\\(\hat{w}\\)に十分近い場合、これらの式は点\\(\hat{w}\\)における誤差関数とその勾配に対する妥当な近似を与える。  
次に誤差関数が最小を取る点\\(w^{\star}\\)の周りでの局所二次近似を考える。この場合勾配はゼロであるため、テイラー展開の一次項は消えて以下のような式になる。  
\\(E(w)=E(w^{\star})+\frac{1}{2}(w-w^{\star})^{T}H(w-w^{\star})\\)  
ここでヘッセ行列Hは点\\(w^{\star}\\)におけるもの。これを幾何学的に解釈するためにヘッセ行列に対する固有値問題を考える。  
Hu=λu  
ここで、H：誤差関数E(w)のヘッセ行列、λ：Hに対応する第i固有値（ラムダの右下には添字のiがついている）、u：対応する正規直行固有ベクトル（λ同様iついている）  
これを使って\\(w-w^{\star}\\)を固有ベクトルの線形結合で表すと  
\\(w-w^{\star}=\sum_{i}\alpha_{i}u_{i}\\)  
のようになる。  
これは座標系を変換したものとみなすことができる。つまり、原点を点\\(w^{\star}\\)に移し、軸をヘッセ行列の固有ベクトル（列ベクトルが\\(u_{i}\\)である直交行列）に沿って回転させたもの。この式を一事項を消したテイラー展開の式に代入し、更に固有値と固有ベクトルを求める式と固有ベクトルとその転置ベクトルの行列積がクロネッカーのデルタを表す式を用いて、  
\\(E(w)=E(w^{\star})+\frac{1}{2}\sum_{i}\lambda_{i}\alpha_{i}\\)  
ヘッセ行列が正定値であるとは任意のベクトルvに対して、  
\\(v^{T}Hv>0\\)  
が成り立つ場合、またその場合に限る。  
固有ベクトル{\\(u_{i}\\)}は完全な基底をなしているため、任意のベクトルvは次のように表せる。  
\\(v=\sum_{i}c_{i}u_{i}\\)  
固有値の式とクロネッカーの対応の式より  
\\(v^{T}Hv=\sum_{i}c_{i}^{2}\lambda_{i}\\)  
となる。したがって全ての固有値\\(\lambda_{i}\\)が正であるとき、そしてそのときに限り、ヘッセ行列Hは正定値となる。  
新しい座標系では基底ベクトルとして固有ベクトル{\\(u_{i}\\)}を用いているため、誤差関数Eの等高線は原点（最小点\\(w^{\star}\\)）を中心とした楕円になる。  
重み空間が１次元の場合、停留点\\(w^{\star}\\)が最小値であるためには、  
\\(\frac{\partial^{2}E}{\partial w^{2}}|_{w^{\star}}>0\\)  
が成り立つ必要がある。  
この結果はD次元の場合には「点\\(w^{\star}\\)におけるヘッセ行列が正定値である」ことに対応する。  
## 5.2.3 Use of gradient information  
セクション5.3で見るように、誤差関数の勾配はバックプロパゲーション法によって効率的に計算することが可能です。
この勾配情報を活用することで、誤差関数の最小値を見つけるまでの速度を大きく改善できる可能性があります。
以下では、その理由を説明します。

誤差関数に対する二次近似（式 (5.28)）では、誤差地形はベクトル \\( \mathbf{b} \\) およびヘッセ行列 \\( \mathbf{H} \\) によって定まります。
これらは合わせて

\\(\frac{W(W+3)}{2}\\) 個の独立な要素を持っています（\\( \mathbf{H} \\) は対称行列であるためです）。 ここで \\( W \\) は重みベクトル \\( \mathbf{w} \\) の次元、つまりネットワーク中の全ての学習可能なパラメータ数です。  
したがって、この二次近似によって誤差関数の最小値の位置を特定するには、 \\( O(W^2) \\) 個のパラメータが必要となり、 それだけの独立した情報が得られるまでは最小点を見つけられないと考えられます。  
勾配情報を使わずにこの情報を集めようとすれば、 関数評価を \\( O(W^2) \\) 回行う必要があり、 さらにそれぞれの評価には \\( O(W) \\) ステップが必要になるので、 最小値を見つけるために必要な総計算量は \\[ O(W^3) \\] になります。  
これと比較して、勾配情報を用いるアルゴリズムでは大きな違いがあります。 なぜなら、勾配 \\( \nabla E \\) を一度評価するだけで \\( W \\) 個の情報が得られるためです。 よって、誤差関数の最小値を見つけるのに必要な勾配評価は \\[ O(W) \\] 回程度で済むと期待されます。 さらに、誤差逆伝播法（バックプロパゲーション）を用いれば、 各勾配評価に必要な計算量も \\( O(W) \\) で済むため、 全体として最小値は \\[ O(W^2) \\] ステップで見つけることが可能になります。  
このような理由から、**勾配情報の活用はニューラルネットワークの訓練における実践的なアルゴリズムの基盤**となっているのです。  

## 5.2.4 Gradient descent optimization  
勾配情報を利用する最も単純な方法は式（5.27）における重みの更新を負の勾配方向への小さなステップとして選ぶこと。つまり  
\\(w^{(\tau+1)}=w^{(\tau)}-\eta\nabla E(w^{(\tau)})\\)  
ここでパラメータ\\(\eta>0\\)は学習率として知られている。各更新のあとに勾配は新しい重みベクトルに対して再評価されて、このプロセスが繰り返される。誤差関数は訓練データセットに関して定義されているため、各ステップでは\\(\nabla E\\)を評価するために訓練セット全体を処理する必要があることに注意。  

データセット全体を一度に使用する手法はバッチ法と呼ばれる。  
今回、各ステップでは重みベクトルが誤差関数を最も急速に減少させる方向へと移動するため、この手法は勾配降下法あるいは最急降下法として知られている。  
このような手法は直感的には合理的に思えるかもしれないが、実際には望ましくないアルゴリズムであることがBishop and Nabneyによって論じられている。  
例えばデメリットとしてはすべてのデータの勾配を計算して覚えておく必要があるから計算時間もメモリ消費も大きくなり、ローカル最小にハマりやすい等が挙げられる。  

バッチ最適化においては、より効率的な手法が存在する。たとえば、**共役勾配法（conjugate gradients）や準ニュートン法（quasi-Newton methods）**であり、これらは単純な勾配降下法よりも頑健で高速である（Gill et al., 1981；Fletcher, 1987；Nocedal and Wright, 1999）。
勾配降下法とは異なり、これらのアルゴリズムでは、重みベクトルが局所あるいは大域的な最小値に到達するまで、誤差関数の値が各反復ステップで常に減少するという特性がある。十分に良い最小値を見つけるためには、勾配ベースのアルゴリズムを複数回実行し、そのたびに異なるランダムな初期点から開始し、独立した検証セットに対する性能を比較する必要があるかもしれない。  
一方で、オンライン版の勾配降下法は、大規模データセットにおけるニューラルネットワークの訓練において実用的であることが示されている（Le Cun et al., 1989）。
独立した観測値の集合に対する最大尤度に基づく誤差関数は、それぞれのデータ点に対する項の総和として次のように表される。  
\\(E(w)=\sum_{n=1}^{N}E_{n}(w)\\)  
（これによって全体ではなく一部のデータを使っても勾配の近似ができるという理屈が成り立っている。）
オンライン勾配降下法（on-line gradient descent）は、**逐次勾配降下法（sequential gradient descent）または確率的勾配降下法（stochastic gradient descent）**としても知られ、1回の更新で1つのデータ点に基づいて重みベクトルを更新する。  
\\(w^{(\tau+1)}=w^{(\tau)}-\eta \nabla E_n(w^{(\tau)})\\)  
この更新はデータを順番にあるいは置き換えありでランダムに選んで巡回することで繰り返される。なおデータ点のバッチに基づいて更新を行う中間的な手法も存在する。  
オンライン手法がバッチ手法と比較して優れている点の一つは、データの冗長性をより効率的に扱えることである。
これを示すために、データセットを取り、それを各データ点を複製して2倍にするという極端な例を考えよう。
この操作は、誤差関数を単に2倍するだけであり、元の誤差関数を使うことと本質的に等価である。
バッチ法では、バッチ誤差関数の勾配を評価するための計算量が2倍になるが、オンライン法では影響を受けない。  
オンライン勾配降下法のもう一つの性質は、局所最小から脱出できる可能性があるという点である。
というのも、データ全体に対する誤差関数の停留点は、個々のデータ点に対する停留点ではないことが一般的だからである。  
非線形最適化アルゴリズムとそのニューラルネットワークへの実用的応用に関しては、Bishop and Nabney（2008）で詳しく議論されている。

